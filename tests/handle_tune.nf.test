nextflow_workflow {

    name "Test Workflow HANDLE_TUNE"
    script "../workflows/handle_tune.nf"
    workflow "HANDLE_TUNE"

    test("Test reproducibility of model tuning") {

        setup {
            run("HANDLE_DATA") {
                script "../workflows/handle_data.nf"
                workflow {
                    """
                    input[0] = "${projectDir}/bin/tests/test_data/dna_experiment/test_with_split.csv"
                    input[1] = "${projectDir}/examples/test.json"
                    input[2] = Channel.of("TESTING")
                    """
                }
            }
        }

        when {
            
            // make so that nf-test will see a singularity cache dir where is usally set to, again interpretation of projectdir has to be done here and not rely on an external config.
            params {
                singularity_cache_dir = "${projectDir}/singularity_cache"
                debug_mode = true


                max_gpus = 0


            }

            // repeating in input[0-1] the same content params.model of params.tune_conf present in the test.config because nf-test does not interpret the ${projectDir} correctly otherwise
            // using both only CPU and only GPU.
            workflow {
                """
                input[0] = "${projectDir}/bin/tests/test_model/dnatofloat_model.py"
                input[1] = "${projectDir}/bin/tests/test_model/dnatofloat_model_cpu.yaml"
                input[2] = HANDLE_DATA.out.data
                """
            }
        }

        then {

            // first chech that the model initialization is always identical
            // then check that the seeds used are the same through randomly picked numbers
            assertAll(
            { assert workflow.success }, 
            { assert snapshot(workflow.out.debug.get(0).get(2)).match("model_weight_initialized") },
            { assert snapshot(workflow.out.debug.get(0).get(3)).match("seeds") }
            )
        }

    }

}
